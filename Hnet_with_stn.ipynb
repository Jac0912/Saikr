{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import data_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.LazyBatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.LazyBatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Downsample_block(nn.Module):\n",
    "    def __init__(self,in_channels=3,out_channels=64,kernel_size = 3,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer1 = DoubleConv(in_channels,out_channels)\n",
    "        self.pooling = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        return self.pooling(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsampler(nn.Module):\n",
    "    #先上采样，然后卷积\n",
    "    def __init__(self, in_channels,out_channels,bilinear = True,*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels , kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)#进行融合裁剪\n",
    "        return self.conv(x)\n",
    "\n",
    "class TransUpsample(nn.Module):\n",
    "    def __init__(self,in_channels,mid_channels,outchannels,kernel_size=2,*args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.up = nn.ConvTranspose2d(in_channels,in_channels,kernel_size)\n",
    "        self.conv = DoubleConv(mid_channels,outchannels)\n",
    "    \n",
    "    def forward(self,x1,x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = nn.functional.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)#进行融合裁剪\n",
    "        return self.conv(x)\n",
    "\n",
    "a = Upsampler(20,30)\n",
    "x = torch.zeros((1,10,30,30))\n",
    "y = torch.zeros((1,10,40,40))\n",
    "a(x,y).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StnAttention(nn.Module):\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.localization = nn.Sequential(\n",
    "            nn.Conv2d(128,64,3),\n",
    "            nn.MaxPool2d(2,stride=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64,128,3),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.fc_loc = nn.Sequential(\n",
    "            nn.Linear(15488,50),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(50,2*3),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        xs = self.localization(x)\n",
    "        xs = xs.view(xs.shape[0],-1)\n",
    "        theta = self.fc_loc(xs)\n",
    "        theta = theta.view(-1,2,3)\n",
    "        grid = nn.functional.affine_grid(theta,x.size())\n",
    "        x = nn.functional.grid_sample(x,grid)\n",
    "        return x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hnet(nn.Module):\n",
    "    def __init__(self,in_channels,num_classes,*args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.downsampler1 = Downsample_block(in_channels)\n",
    "        self.downsampler2 = Downsample_block(64,128)\n",
    "        #self.downsampler3 = Downsample_block(128,256)\n",
    "        self.bottom = DoubleConv(128,256)\n",
    "        self.upsampler1 = TransUpsample(256,384,64)\n",
    "        #self.upsampler2 = Upsampler(384,64)\n",
    "        #//self.upsampler3 = Upsampler(128,64)\n",
    "        \n",
    "        \n",
    "        self.restrans = TransUpsample(64,128,64,2)\n",
    "        self.resconv = nn.Conv2d(64,num_classes,1)\n",
    "        self.res_upsampler = nn.Upsample(scale_factor=2,mode='bilinear',align_corners=True)\n",
    "        \n",
    "        self.attention = StnAttention()\n",
    "\n",
    "    def cut_tensor(self,x, target_tensor):\n",
    "        shape_tgt = target_tensor.shape[-1]  # 图片是正方形的\n",
    "        shape_src = x.shape[-1]\n",
    "        \n",
    "        # 计算裁剪的偏移量，确保为整数\n",
    "        delta = int((shape_src - shape_tgt) / 2)\n",
    "        \n",
    "        # 如果 `shape_src - shape_tgt` 是奇数，调整裁剪范围\n",
    "        if (shape_src - shape_tgt) % 2 != 0:\n",
    "            end_delta = delta + 1\n",
    "        else:\n",
    "            end_delta = delta\n",
    "        \n",
    "        return x[:, :, delta:shape_src-end_delta, delta:shape_src-end_delta]\n",
    "    \n",
    "    def forward(self,_):\n",
    "        x1 = self.downsampler1(_)\n",
    "        #x1的形状为 100，100\n",
    "        x2 = self.downsampler2(x1)\n",
    "        x3 = self.bottom(x2)\n",
    "        x4 = self.upsampler1(x3,x2)\n",
    "        x5 = self.restrans(x4,x1)\n",
    "        return self.res_upsampler(self.resconv(x5))\n",
    "\n",
    "x = torch.zeros((1,3,200,200))\n",
    "a = Hnet(3,4)\n",
    "a(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "train_data,test_data = data_reader.get_train_and_test_data()\n",
    "\n",
    "net = Hnet(3,4)\n",
    "net = net.to('cuda:0')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "train_iter = torch.utils.data.DataLoader(train_data,16,True,num_workers=4)\n",
    "test_iter = torch.utils.data.DataLoader(test_data,64,num_workers=4)\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(net.parameters(),1e-4,weight_decay=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "losses,train_acc = [],[]\n",
    "test_acc = []\n",
    "for epoch in range(epochs):\n",
    "    net.train()\n",
    "    for x,y in train_iter:\n",
    "        x,y = x.to('cuda:0'),y.to('cuda:0')\n",
    "        optimizer.zero_grad()\n",
    "        pre = net(x)\n",
    "        loss = loss_fn(pre,y.squeeze(1).long()).mean(1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        losses.append(loss)\n",
    "        train_acc.append(${func computes acc})\n",
    "        test_acc.append(${func computes acc})\n",
    "    print(f\"epoch: {epoch} loss: {losses[-1]} train_acc:{train_acc[-1]} test_acc: {test_acc[-1]}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
